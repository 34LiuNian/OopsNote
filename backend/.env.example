# Copy to .env (same directory as this file)
# Backend reads this file on startup (python-dotenv).

# OpenAI-compatible gateway (e.g. local gateway)
OPENAI_BASE_URL=http://127.0.0.1:23333/v1
OPENAI_API_KEY=your-gateway-key

# Optional: explicit gateway auth header (used by /models proxy and startup prefetch)
# python-dotenv supports ${VAR} interpolation
OPENAI_AUTH_HEADER_NAME=Authorization
OPENAI_AUTHORIZATION=Bearer ${OPENAI_API_KEY}

# Optional defaults
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.2

# Optional: cap output length
# OPENAI_MAX_TOKENS=900

# Optional Gemini fallback
# GEMINI_API_KEY=your-gemini-key
# GEMINI_MODEL=gemini-1.5-flash
# GEMINI_TEMPERATURE=0.2

# Enable multi-agent orchestrator
ENABLE_MULTI_AGENT=true

# Optional feature toggles
# ENABLE_SCANNER=true

# Optional task persistence
# PERSIST_TASKS=true
# TASKS_DIR=./storage/tasks

# Optional: mark tasks stuck in processing as failed (seconds)
# TASK_STALE_SECONDS=600

# Optional app logs
# APP_LOG_LEVEL=INFO
# AI_DEBUG_LLM=false

# Optional: agent TOML config path
# AGENT_CONFIG_PATH=./agent_config.toml
